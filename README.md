# Vocabler Backend Server
This project contains all necessary code + data to setup a Vocabler backend instance.

- [Vocabler Backend Server](#vocabler-backend-server)
  - [Project Description](#project-description)
  - [Local Setup Guide](#local-setup-guide)
    - [MongoDB Collections](#mongodb-collections)
    - [Stripe](#stripe)
      - [Test API Keys for Stripe:](#test-api-keys-for-stripe)
    - [Environment Variables](#environment-variables)
  - [Running the Server](#running-the-server)
  - [Optional Scripts](#optional-scripts)
    - [WordNet Importer](#wordnet-importer)
    - [Book Parser](#book-parser)


## Project Description
The backend server is responsible for managing user requests, executing them, and manipulating the database.
In this project, the backend offers 5 different routes which take care of their respective functionality:

1. [``/users``](./src/routes/userRoute.js): registration, login/logout, profile manipulation (username, password, profile pic, etc.).
2. [``/books``](./src/routes/bookRoute.js): getting book information.
3. [``/review``](./src/routes/reviewRoute.js): Creating, getting, updating, and deleting reviews.
4. [``/study``](./src/routes/studyRoute.js): Adding and manipulating decks, fetching and rating study flashcards, and setting the study configuration settings.
5. [``/payment``](./src/routes/paymentRoute.js): Fetching customer info and cancelling a subscription.


## Local Setup Guide
Certain pre-requisites need to be fulfilled in order to have a fully functioning backend instance.\
**This part can be ignored, since we supply a remote database, stripe API keys and .env file.\
For fully functioning subscription functionality, you will need to setup your own Stripe account. For more details, check out [this subsection](#stripe)**

### MongoDB Collections
Books and lexicalEntries collections are pre-generated by us and can be found in [data](./data/collections/). These should be inserted in the local database unmodified, since books reference lexicalEntries for vocabulary.

### Stripe
We use stripe payment processing to handle user payment details. 
In this README, we supply the test keys (which goes against common practice, but we'll make an exception for this course).

The test keys will suffice to complete a checkout in the frontend and cancel subscriptions, however **the plan upgrading will not work**.
This is because the webhook target address must be manually set from the Stripe dashboard which can only be accessed with login credentials.
**Please understand that we will not share these login credentials, since one of our team members created the account using their personal and financial data.**

If the full subscription functionality is desired, then you will need to create your own stripe account to access the dashboard and set the webhook setting to the remote address + port of the backend ```{{address}}:{{port}}/webhook```. The secret + publishable keys in the backend will have to be set using that account, as well.

#### Test API Keys for Stripe:
 STRIPE_PUBLISHABLE_KEY="pk_test_51NMDgsI7ip5F2O7EwAO0yAKwPTK4IOn2G003XhKOs5SAHSPK6A5qFrVmsqschGqH2tkwxk2skF003ehOE7V75k3E00glxamR2j"
STRIPE_SECRET_KEY="sk_test_51NMDgsI7ip5F2O7EyhrY5jt1xO9rbegkDkkI1Fy6OXiIKw3JRjONWaZ8oK1zwyF7p7xsf66G39XZquXoxVB2tCmX00X3jmyeCQ"

### Environment Variables
**NOTE:** We have uploaded the .env to the repo, so you (the tester), will not need to set anything.
An .env file is required to run the server and should contain all necessary variables.
In most cases, the server will use a default value if not provided by the .env file. Therefore, only the non-optional values in this table are absolutely required for the .env file:

| Name                   | Optional | Example                     | Default                     |
|------------------------|----------|-----------------------------|-----------------------------|
| DB_ADDRESS             | [x]        | 'mongodb://localhost:27017' | 'mongodb://localhost:27017' |
| DB_NAME                | [x]        | 'myDatabase'                | 'vocablerDB'                |
| JWT_SECRET             | [ ]        | 'Very secret secret'        |                             |
| JWT_EXPIRATION_TIME    | [x]        | 86400                       | 86400                       |
| PROCESS_ENV            | [x]        | 'production'                | 'development'               |
| PORT                   | [x]        | 12345                       | 8081                        |
| SALT_ROUNDS            | [x]        | 10                          | 10                          |
| STRIPE_SECRET_KEY      | [ ]        | 'sk_test_51N...'            |                             |
| STRIPE_PUBLISHABLE_KEY | [ ]        | 'pk_test_51N...'            |                             |

## Running the Server
Before running, you will need to install all necessary packages
```
npm install
```
You can then run the server using one of the following: 
```
#start normally
npm run start

#start with nodemon
npm run dev
```
In case anything went wrong during runtime, error logs can be viewed in vocabler.log.

The server is now active and ready.

## Optional Scripts
The following are descriptions for the python scripts.

These scripts were written by us to parse the books and map each word to a lexical entry in WordNet. They do not need to be run for testing, but in case you are interested, we will go into further detail in the next subsections.

### WordNet Importer
Using pythons [nltk package](https://www.nltk.org/), we can load all synsets in the entire WordNet, extract definitions, examples, and even map them to frequencies in literature. Then we save each object as a lexical entry in our local database.

**Note**: You may need to edit some constants such as the database address/name if a different location is desired.

### Book Parser
Pre-requisite: wn_importer.py should be run first to make sure that the lexicalentries collection exists.

The script opens the content of the epub and tokenizes the entire script into sentences. For each sentence, each word is assigned a PoS (part of speech) and checked in lexical entries for a matching definition. If it exists, then the script saves the lexical entry ID for the word and continues.
Other infos like frequency of the word within the book are also tracked.

Finally the book is saved to the book collection in the database.